{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries.\n",
    "import  tensorflow as tf\n",
    "import  keras\n",
    "\n",
    "from    keras.models import Sequential\n",
    "from    keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Dropout, BatchNormalization\n",
    "from    keras.utils import to_categorical\n",
    "from    keras.callbacks import EarlyStopping\n",
    "\n",
    "# Helper libraries.\n",
    "import  numpy as np\n",
    "import  sklearn\n",
    "from    sklearn.model_selection import train_test_split\n",
    "\n",
    "# Libraries for plotting.\n",
    "import  matplotlib\n",
    "import  matplotlib.pyplot as plt\n",
    "from    PIL import Image\n",
    "from    IPython.display import display\n",
    "\n",
    "# Other\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries.\n",
    "import  tensorflow as tf\n",
    "import  keras\n",
    "\n",
    "from    keras.models import Sequential\n",
    "from    keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Dropout, BatchNormalization\n",
    "from    keras.utils import to_categorical\n",
    "from    keras.callbacks import EarlyStopping\n",
    "\n",
    "# Helper libraries.\n",
    "import  numpy as np\n",
    "import  sklearn\n",
    "from    sklearn.model_selection import train_test_split\n",
    "\n",
    "# Libraries for plotting.\n",
    "import  matplotlib\n",
    "import  matplotlib.pyplot as plt\n",
    "from    PIL import Image\n",
    "from    IPython.display import display\n",
    "\n",
    "# Other\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for GPU and determine what GPU we have.\n",
    "# Modified by student to remove warningen caused on local machine.\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if not tf.config.list_physical_devices('GPU'):\n",
    "\tprint(\"No GPU was detected. CNNs can be very slow without a GPU.\")\n",
    "\tif IN_COLAB:\n",
    "\t\tprint(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
    "else:\n",
    "\tos.system('nvidia-smi -L')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpu_devices:\n",
    "    details = tf.config.experimental.get_device_details(gpu_devices[0])\n",
    "    compute_capability=details.get('compute_capability')\n",
    "    print(\"Compute capability:\",compute_capability)\n",
    "    if compute_capability[0]>6:\n",
    "        print(\"Turn on mixed_float16\")\n",
    "        policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "        tf.keras.mixed_precision.set_global_policy(policy)\n",
    "else:\n",
    "    print(\"No GPU detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify GPU memory allocator to try to prevent full GPU memory.\n",
    "# This can in some cases be counter productive!\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Keras Tuner via PIP (if in colab).\n",
    "if IN_COLAB:\n",
    "    os.system('pip install keras_tuner')\n",
    "\n",
    "import keras_tuner\n",
    "print('Keras Tuner version:', keras_tuner.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "# Function to load a ZIP file from Google Drive.\n",
    "def load_zip_file(zip_path):\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            # Extract to a specific directory\n",
    "            zip_ref.extractall('/content/Model/dataset')\n",
    "            print(f\"ZIP file '{zip_path}' loaded successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: ZIP file not found at '{zip_path}'.\")\n",
    "    except zipfile.BadZipFile:\n",
    "        print(f\"Error: Invalid ZIP file at '{zip_path}'.\")\n",
    "\n",
    "# If in Colab, load the ZIP file from Google Drive.\n",
    "if IN_COLAB:\n",
    "    print(\"Loading and extracting dataset ZIP-file form Drive.\")\n",
    "    load_zip_file(\"drive/MyDrive/Studies/KTH/Courses/II143X/Model Creation/Data/output_frames.zip\")\n",
    "else:\n",
    "    print(\"No Colab detected, using pre-existing files.\")\n",
    "\n",
    "extracted_path = \"output_frames\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_array = []\n",
    "dataset_classes = []\n",
    "num_of_classes = 0\n",
    "for directory in os.listdir(extracted_path):\n",
    "    if os.path.isdir(os.path.join(extracted_path, directory)):\n",
    "        num_of_classes += 1\n",
    "        for file in os.listdir(os.path.join(extracted_path, directory)):\n",
    "            if file.endswith(\".txt\"):\n",
    "                dataset_array.append(os.path.join(extracted_path, directory, file))\n",
    "                dataset_classes.append(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset_array))\n",
    "print(len(dataset_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(dataset_array).shape)\n",
    "print(np.array(dataset_classes).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_array[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processTXTFile(file_path):\n",
    "    print(file_path)\n",
    "    with open(file_path, 'r') as file:\n",
    "        mfcc_txt = file.read()\n",
    "        mfcc_data = np.array(eval(mfcc_txt)).T\n",
    "    return mfcc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "def plot_mfcc(mfccs):\n",
    "    print(f\"MFCC shape: {mfccs.shape}\")\n",
    "    mfccs = mfccs\n",
    "    librosa.display.specshow(mfccs, x_axis='time', sr=16000)\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title('MFCC')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('MFCC Coefficient')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_array[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(0, dataset_array[0].__len__()):\n",
    "#    plot_mfcc(dataset_array[0][i])\n",
    "plot_mfcc(processTXTFile(dataset_array[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Split up the data into training and testing data.\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(dataset_array, dataset_classes, test_size=0.3, train_size=0.7)\n",
    "\n",
    "# Further split the temporary set into validation and test sets.\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, train_size=0.5)\n",
    "\n",
    "# Encode string labels to integers\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_val_encoded = label_encoder.transform(y_val)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "print(num_of_classes)\n",
    "\n",
    "y_train_cat = to_categorical(y_train_encoded, num_classes=num_of_classes)\n",
    "y_val_cat = to_categorical(y_val_encoded, num_classes=num_of_classes)\n",
    "y_test_cat = to_categorical(y_test_encoded, num_classes=num_of_classes)\n",
    "\n",
    "print(f\"Train data shape: {np.array(x_train).shape}\")\n",
    "print(f\"Train labels shape: {y_train_cat.shape}\")\n",
    "print(f\"Validation data shape: {np.array(x_val).shape}\")\n",
    "print(f\"Validation labels shape: {y_val_cat.shape}\")\n",
    "print(f\"Test data shape: {np.array(x_test).shape}\")\n",
    "print(f\"Test labels shape: {y_test_cat.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tf = tf.data.TextLineDataset(x_train)\n",
    "\n",
    "def processTXTFile_tf(file_path):\n",
    "    file_path = file_path.numpy().decode('utf-8')\n",
    "    data = np.concatenate(processTXTFile(file_path), axis=0)\n",
    "    return data\n",
    "\n",
    "x_train_tf = x_train_tf.interleave(\n",
    "    lambda file: tf.data.Dataset.from_tensors(tf.py_function(processTXTFile_tf, [file], [tf.float32])),\n",
    "    cycle_length=1,  # Controls how many files are read in parallel (set to 1 for strict on-demand processing)\n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "# Create a dataset for y_train\n",
    "y_train_tf = tf.data.Dataset.from_tensor_slices(y_train_cat)\n",
    "\n",
    "# Combine x_train_tf and y_train_tf into a single dataset\n",
    "train_tf = tf.data.Dataset.zip((x_train_tf, y_train_tf))\n",
    "\n",
    "# Batch and prefetch the dataset\n",
    "train_tf = train_tf.batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_tf.batch(1).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val_tf = tf.data.TextLineDataset(dataset_array)\n",
    "\n",
    "def processTXTFile_tf(file_path):\n",
    "    file_path = file_path.numpy().decode('utf-8')\n",
    "    data = np.concatenate(processTXTFile(file_path), axis=0)\n",
    "    return data\n",
    "\n",
    "x_val_tf = x_val_tf.interleave(\n",
    "    lambda file: tf.data.Dataset.from_tensors(tf.py_function(processTXTFile_tf, [file], [tf.float32])),\n",
    "    cycle_length=1,  # Controls how many files are read in parallel (set to 1 for strict on-demand processing)\n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "# Create a dataset for y_train\n",
    "y_val_tf = tf.data.Dataset.from_tensor_slices(y_val_cat)\n",
    "\n",
    "# Combine x_train_tf and y_train_tf into a single dataset\n",
    "val_tf = tf.data.Dataset.zip((x_val_tf, y_val_tf))\n",
    "\n",
    "# Batch and prefetch the dataset\n",
    "val_tf = val_tf.batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train_cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Classes: {format(dataset_classes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the sequential model and all layers\n",
    "\n",
    "input_shape = (25, 16, 1)\n",
    "\n",
    "sequential_model = Sequential([\n",
    "    Conv2D(filters = 64, kernel_size = (3,3), activation = \"relu\", input_shape = input_shape, padding = 'same'),\n",
    "    Conv2D(filters = 64, kernel_size = (2,2), activation = \"relu\", input_shape = input_shape, padding = 'same'),\n",
    "    MaxPool2D(pool_size=(2,2)),\n",
    "    Dropout(0.4),\n",
    "    Conv2D(filters = 128, kernel_size = (3,3), activation = \"relu\", input_shape = input_shape, padding = 'same'),\n",
    "    MaxPool2D(pool_size=(2,2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation = \"relu\"),\n",
    "    Dense(64, activation = \"relu\"),\n",
    "    Dropout(0.6),\n",
    "    Dense(units = num_of_classes, activation = \"softmax\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_model.compile(optimizer = \"Adam\", loss = \"categorical_crossentropy\", metrics = ['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printHistory(history):\n",
    "  epochs = len(history.history['loss']) # Collect the number of epochs run based on the amount of loss value under history.\n",
    "\n",
    "  epochrange = range(1, epochs + 1)\n",
    "  train_acc = history.history['categorical_accuracy']\n",
    "  val_acc = history.history['val_categorical_accuracy']\n",
    "\n",
    "  train_loss = history.history['loss']\n",
    "  val_loss = history.history['val_loss']\n",
    "\n",
    "  plt.plot(epochrange, train_acc, 'bo', label='Training acc')\n",
    "  plt.plot(epochrange, val_acc, 'b', label='Validation acc')\n",
    "  plt.title('Training and validation accuracy (modell 1)')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "  plt.plot(epochrange, train_loss, 'bo', label='Training loss')\n",
    "  plt.plot(epochrange, val_loss, 'b', label='Validation loss')\n",
    "  plt.title('Training and validation loss (modell 1)')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.legend()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitModel(model, train_tf, val_tf, epochs, batch_size, doPrintHistory):\n",
    "    history = model.fit(train_tf, validation_data=val_tf, epochs=epochs, batch_size=batch_size)\n",
    "    if doPrintHistory: printHistory(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check size of what train_tf is\n",
    "print(train_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitModel(sequential_model, train_tf, val_tf, 10, 32, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
