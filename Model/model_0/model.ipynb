{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d6YB6Mh93Vc"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzLKpmZICaWN"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-03-24 21:03:06.228598: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-03-24 21:03:07.520326: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-03-24 21:03:07.798403: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742846587.950773  108097 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742846587.998638  108097 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-24 21:03:08.645948: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZKNhx8x_1jq",
        "outputId": "6e727e5e-e91a-46c6-bb9f-57c922a20cab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.18.0\n"
          ]
        }
      ],
      "source": [
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test for GPU and determine what GPU we have.\n",
        "# Modified by student to remove warningen caused on local machine.\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "gpu_devices = tf.config.list_physical_devices('GPU')\n",
        "\n",
        "if gpu_devices:\n",
        "    details = tf.config.experimental.get_device_details(gpu_devices[0])\n",
        "    compute_capability=details.get('compute_capability')\n",
        "    print(\"Compute capability:\",compute_capability)\n",
        "    if compute_capability[0]>6:\n",
        "        print(\"Turn on mixed_float16\")\n",
        "        policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
        "        tf.keras.mixed_precision.set_global_policy(policy)\n",
        "else:\n",
        "    print(\"No GPU detected.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Modify GPU memory allocator to try to prevent full GPU memory.\n",
        "# This can in some cases be counter productive!\n",
        "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VULHpvbVl0N"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ymcbpqPdLJxW"
      },
      "outputs": [],
      "source": [
        "def get_file_size(file_path):\n",
        "    size = os.path.getsize(file_path)\n",
        "    return size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_vfRLdF_LKUK"
      },
      "outputs": [],
      "source": [
        "def convert_bytes(size, unit=None):\n",
        "    if unit == \"KB\":\n",
        "        return print('File size: ' + str(round(size / 1024, 3)) + ' Kilobytes')\n",
        "    elif unit == \"MB\":\n",
        "        return print('File size: ' + str(round(size / (1024 * 1024), 3)) + ' Megabytes')\n",
        "    else:\n",
        "        return print('File size: ' + str(size) + ' bytes')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArL366zjd4Xc"
      },
      "source": [
        "# Importa Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieeCS0L0d4Xd"
      },
      "outputs": [],
      "source": [
        "DATASET_PATH = \"/content/drive/MyDrive/seconds_per_frame0.50,overlap0.25,mfccs40_.csv\"\n",
        "df = pd.read_csv(DATASET_PATH, header=None)                     # No header in your format\n",
        "dataset = df.iloc[:, :-1].to_numpy(dtype=np.float32)            # All but last column as float16\n",
        "labels_set = df.iloc[:, -1].to_numpy(dtype=str)                 # Last column as string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIH5p603d4Xd",
        "outputId": "9694389a-faaf-40c2-ae76-e952ccbfffc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset shape: (583554, 40, 16, 1)\n",
            "Labels shape: (583554,)\n",
            "Input shape: (40, 16, 1)\n"
          ]
        }
      ],
      "source": [
        "INPUT_SHAPE = (40, 16, 1)\n",
        "dataset = dataset.reshape(dataset.shape[0], INPUT_SHAPE[0], INPUT_SHAPE[1], INPUT_SHAPE[2])\n",
        "\n",
        "print(f\"Dataset shape: {dataset.shape}\")\n",
        "print(f\"Labels shape: {labels_set.shape}\")\n",
        "print(f\"Input shape: {INPUT_SHAPE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ap-ud1Jbd4Xe",
        "outputId": "1568c64c-2c52-4fe6-8208-55bf8b5712ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set length: 373474\n",
            "Validation set length: 93369\n",
            "Testing set length: 116711\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(dataset, labels_set, test_size=0.2, random_state=42, stratify=labels_set)\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
        "\n",
        "# Print the lengths of the training, validation, and testing sets.\n",
        "print(f\"Training set length: {len(x_train)}\")\n",
        "print(f\"Validation set length: {len(x_val)}\")\n",
        "print(f\"Testing set length: {len(x_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2p_MTiid4Xf"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Calculate class weights (based on training data).\n",
        "class_weight = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "dist_class_weight = dict(enumerate(class_weight))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10khbK5Qd4Xf",
        "outputId": "66f9130e-a73a-4bca-e8af-4c6f93320520"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classes: ['Background_noise' 'Car' 'Commercial_vehicles' 'Motorcycle']\n"
          ]
        }
      ],
      "source": [
        "print(f\"Classes: {np.unique(labels_set)}\")\n",
        "print(f\"Class weights: {dist_class_weight}\")\n",
        "\n",
        "# Print out the amount of each class.\n",
        "unique, counts = np.unique(y_train, return_counts=True)\n",
        "print(f\"Training class distribution: {dict(zip(unique, counts))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "qEXSIf2Qd4Xh"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'y_train' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[24], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Hot end code the labels.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m label_encoder \u001b[38;5;241m=\u001b[39m LabelEncoder()\n\u001b[0;32m----> 6\u001b[0m y_train \u001b[38;5;241m=\u001b[39m to_categorical(label_encoder\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43my_train\u001b[49m))\n\u001b[1;32m      7\u001b[0m y_test \u001b[38;5;241m=\u001b[39m to_categorical(label_encoder\u001b[38;5;241m.\u001b[39mfit_transform(y_test))\n\u001b[1;32m      8\u001b[0m y_val \u001b[38;5;241m=\u001b[39m to_categorical(label_encoder\u001b[38;5;241m.\u001b[39mfit_transform(y_val))\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y_train' is not defined"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Hot end code the labels.\n",
        "label_encoder = LabelEncoder()\n",
        "y_train = to_categorical(label_encoder.fit_transform(y_train))\n",
        "y_test = to_categorical(label_encoder.fit_transform(y_test))\n",
        "y_val = to_categorical(label_encoder.fit_transform(y_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59veuiEZCaW4"
      },
      "source": [
        "## Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8preOUYXd4Xk"
      },
      "outputs": [],
      "source": [
        "def compileModel(model, learning_rate = 0.001):\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "        loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "        metrics=['categorical_accuracy']\n",
        "    )\n",
        "    model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "    \n",
        "def printConfusionMatrix(model, x_test, y_test, labels_set):\n",
        "    y_train_int = np.argmax(y_train, axis=1)  # Convert one-hot to integer labels for y_train\n",
        "    y_test_int = np.argmax(y_test, axis=1)  # Convert one-hot to integer labels for y_test\n",
        "\n",
        "    y_pred_prob = model.predict(x_test)\n",
        "\n",
        "    y_pred = np.argmax(y_pred_prob, axis=1)  # Convert softmax probabilities to class indices\n",
        "\n",
        "    # Ensure that the labels are in 1D arrays (class indices) before passing to confusion_matrix\n",
        "    print(f\"y_train_int shape: {y_train_int.shape}\")\n",
        "    print(f\"y_test_int shape: {y_test_int.shape}\")\n",
        "    print(f\"y_pred shape: {y_pred.shape}\")\n",
        "\n",
        "    # Define your class labels (ensure they match the number of classes)\n",
        "    class_names = np.unique(labels_set)  # Assuming labels_set contains class names\n",
        "\n",
        "    # Generate confusion matrix\n",
        "    cm = confusion_matrix(y_test_int, y_pred)\n",
        "\n",
        "    # Visualize the confusion matrix with proper labels\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print training history\n",
        "# Function used for printing two plots, one for accuracy and one for loss.\n",
        "# To be used with the history object from the fit method\n",
        "def printHistory(history):\n",
        "  # Collect the number of epochs run based on the amount of loss value under history.\n",
        "  epochs = len(history.history['loss'])\n",
        "\n",
        "  epochrange = range(1, epochs + 1)\n",
        "  train_acc = history.history['categorical_accuracy']\n",
        "  val_acc = history.history['val_categorical_accuracy']\n",
        "\n",
        "  train_loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "\n",
        "  plt.plot(epochrange, train_acc, 'bo', label='Training acc')\n",
        "  plt.plot(epochrange, val_acc, 'b', label='Validation acc')\n",
        "  plt.title('Training and validation accuracy (modell 1)')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  plt.plot(epochrange, train_loss, 'bo', label='Training loss')\n",
        "  plt.plot(epochrange, val_loss, 'b', label='Validation loss')\n",
        "  plt.title('Training and validation loss (modell 1)')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# Fit model function used for convinience when fitting the model multiple times\n",
        "def fitModel(model, x_train, y_train, x_val, y_val, epochs, doPrintHistory, class_weight, verbose, batch_size):\n",
        "\n",
        "    # Define early stopping callback.\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "    history = model.fit(\n",
        "        x=x_train,\n",
        "        y=y_train,\n",
        "        validation_data=(x_val, y_val),\n",
        "        epochs=epochs,\n",
        "        verbose=verbose,\n",
        "        class_weight=class_weight,\n",
        "        batch_size=batch_size,\n",
        "        callbacks=[early_stopping],\n",
        "    )\n",
        "    # Print history\n",
        "    if doPrintHistory: printHistory(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create, train, compile, test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ODch-OFCaW4"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import InputLayer\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "model = keras.Sequential([\n",
        "    InputLayer(shape=INPUT_SHAPE),\n",
        "\n",
        "    Conv2D(filters=32, kernel_size=3, activation=\"relu\", padding='same'),\n",
        "    MaxPooling2D(),\n",
        "\n",
        "    Conv2D(filters=32, kernel_size=3, activation=\"relu\", padding='same'),\n",
        "    MaxPooling2D(),\n",
        "\n",
        "    # Feature Pooling (Combining Max & Average Pooling)\n",
        "    Flatten(),\n",
        "\n",
        "    # Fully Connected Layer\n",
        "    Dense(32, activation=\"relu\", kernel_regularizer=l2(0.001)),\n",
        "\n",
        "    # Output Layer (Softmax for multi-class classification)\n",
        "    Dense(units=len(np.unique(labels_set)), activation=\"softmax\") # Softmax barely increase size.\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "xvwvpA64CaW_",
        "outputId": "ea8350af-94fe-4b17-edac-315502c165e6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">40,992</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">165</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_10 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_10 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_11 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)      │         \u001b[38;5;34m9,248\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_11 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m32\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_5 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │        \u001b[38;5;34m40,992\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │           \u001b[38;5;34m165\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">50,725</span> (198.14 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m50,725\u001b[0m (198.14 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">50,725</span> (198.14 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m50,725\u001b[0m (198.14 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "compileModel(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fitModel(model, x_train, y_train, x_val, y_val, 8, True, dist_class_weight, 1, 32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the model on the test set.\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
        "print(f\"Test accuracy: {test_acc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "printConfusionMatrix(model, x_test, y_test, labels_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Save & Compress model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Keras (h5) model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4n4_BRNKLRwo",
        "outputId": "c7ba628e-de8f-47d0-88f3-f01b0681a0ab"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File size: 0.226 Megabytes\n"
          ]
        }
      ],
      "source": [
        "KERAS_MODEL_NAME = \"tf_model_mini.h5\" # Try using .keras instead.\n",
        "model.save(KERAS_MODEL_NAME)\n",
        "convert_bytes(get_file_size(KERAS_MODEL_NAME), \"MB\")\n",
        "keras_model_size = get_file_size(KERAS_MODEL_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TF Lite model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NY57t7EwCW8P",
        "outputId": "918e15da-7546-4eac-f6de-630434e2aa41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpor3gw8sx/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpor3gw8sx/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved artifact at '/tmp/tmpor3gw8sx'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 40, 16, 1), dtype=tf.float32, name='keras_tensor_182')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 5), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  139938721834160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139938716697872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139938716698224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139938716705968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139938716710368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139938716706672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139938716709664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139938716707728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "W0000 00:00:1742847307.799955  108097 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
            "W0000 00:00:1742847307.800758  108097 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
            "2025-03-24 21:15:07.804820: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpor3gw8sx\n",
            "2025-03-24 21:15:07.805872: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
            "2025-03-24 21:15:07.805887: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmpor3gw8sx\n",
            "2025-03-24 21:15:07.814848: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
            "2025-03-24 21:15:07.868095: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmpor3gw8sx\n",
            "2025-03-24 21:15:07.880267: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 75494 microseconds.\n"
          ]
        }
      ],
      "source": [
        "TF_LITE_MODEL_FILE_NAME = \"tf_lite_model.tflite\"\n",
        "tf_lite_converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = tf_lite_converter.convert()\n",
        "tflite_model_name = TF_LITE_MODEL_FILE_NAME\n",
        "open(tflite_model_name, \"wb\").write(tflite_model)\n",
        "convert_bytes(get_file_size(TF_LITE_MODEL_FILE_NAME), \"KB\")\n",
        "tflite_file_size = get_file_size(TF_LITE_MODEL_FILE_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_gttI3-M45M"
      },
      "source": [
        "### Compress Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJXJuGLFGfjB",
        "outputId": "4321dcd8-c7ce-4564-eedd-66a8ff374a06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input Shape: [ 1 40 16  1]\n",
            "Input Type: <class 'numpy.float32'>\n",
            "Output Shape: [1 5]\n",
            "Output Type: <class 'numpy.float32'>\n"
          ]
        }
      ],
      "source": [
        "# Checkout model.\n",
        "interpreter = tf.lite.Interpreter(model_path = TF_LITE_MODEL_FILE_NAME)\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "print(\"Input Shape:\", input_details[0]['shape'])\n",
        "print(\"Input Type:\", input_details[0]['dtype'])\n",
        "print(\"Output Shape:\", output_details[0]['shape'])\n",
        "print(\"Output Type:\", output_details[0]['dtype'])\n",
        "\n",
        "# Evaluate model on test set.\n",
        "interpreter.allocate_tensors()\n",
        "input_index = input_details[0]['index']\n",
        "output_index = output_details[0]['index']\n",
        "tflite_accuracy = 0\n",
        "for i in range(len(x_test)):\n",
        "    input_data = x_test[i]\n",
        "    input_data = np.array(input_data, dtype=np.float32)\n",
        "    input_data = np.expand_dims(input_data, axis=0)\n",
        "    interpreter.set_tensor(input_index, input_data)\n",
        "    interpreter.invoke()\n",
        "    output = interpreter.get_tensor(output_index)\n",
        "    tflite_accuracy += (np.argmax(output) == np.argmax(y_test[i]))\n",
        "tflite_accuracy = tflite_accuracy / len(x_test)\n",
        "print(\"TFLite accuracy: \", tflite_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UC7afqBd4Xo",
        "outputId": "b0f959a8-d1a6-4ea8-fedf-ee858062a8b7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpl1udczw1/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpl1udczw1/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved artifact at '/tmp/tmpl1udczw1'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 40, 16, 1), dtype=tf.float32, name='input_layer_5')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 5), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  139938716033344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139938716038272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139938716078448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139938716082496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139938716213392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139938716211456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139938718061968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139938716215504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "W0000 00:00:1742847308.800492  108097 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
            "W0000 00:00:1742847308.800546  108097 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
            "2025-03-24 21:15:08.800779: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpl1udczw1\n",
            "2025-03-24 21:15:08.801414: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
            "2025-03-24 21:15:08.801431: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmpl1udczw1\n",
            "2025-03-24 21:15:08.806285: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
            "2025-03-24 21:15:08.843990: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmpl1udczw1\n",
            "2025-03-24 21:15:08.854394: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 53616 microseconds.\n"
          ]
        }
      ],
      "source": [
        "INT8_MODEL_FILE_NAME = \"tf_lite_int8_model.tflite\"\n",
        "\n",
        "# Load your existing model\n",
        "model = tf.keras.models.load_model(KERAS_MODEL_NAME)\n",
        "\n",
        "# Convert the model to TensorFlow Lite format with quantization\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_types = [tf.uint8]\n",
        "\n",
        "# Ensure that the input and output types are uint8\n",
        "def representative_dataset_gen():\n",
        "    for i in range(len(x_test)):\n",
        "        # Get sample input data as a numpy array in a method of your choosing\n",
        "        yield [x_test[i]]\n",
        "\n",
        "converter.representative_dataset = representative_dataset_gen\n",
        "tflite_quant_model = converter.convert()\n",
        "\n",
        "# Save the quantized model\n",
        "with open(INT8_MODEL_FILE_NAME, 'wb') as f:\n",
        "    f.write(tflite_quant_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Quantized Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_p-Ioynd4Xo",
        "outputId": "ba34cf63-3090-49d2-982a-77261299ba35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow Lite model output: [[inf inf inf inf inf]]\n"
          ]
        }
      ],
      "source": [
        "# Load and test the quantized model\n",
        "interpreter = tf.lite.Interpreter(model_path=INT8_MODEL_FILE_NAME)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Test the TensorFlow Lite model on random data from x_test\n",
        "input_shape = input_details[0]['shape']\n",
        "output_shape = output_details[0]['shape']\n",
        "\n",
        "# Test the TensorFlow Lite model on random data from x_test\n",
        "interpreter.invoke()\n",
        "tflite_results = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "print(\"TensorFlow Lite model output:\", tflite_results)\n",
        "print(\"TensorFlow Lite model output shape:\", tflite_results.shape)\n",
        "\n",
        "# Print file size of the TensorFlow Lite model\n",
        "convert_bytes(get_file_size(INT8_MODEL_FILE_NAME), \"KB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Convert Quantized Model into CPP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBs0q1dCd4Xo"
      },
      "outputs": [],
      "source": [
        "INT8_MODEL_CPP_FILE_NAME = \"pipeline_int8_model_data.cpp\"\n",
        "!xxd -i {INT8_MODEL_FILE_NAME} > {INT8_MODEL_CPP_FILE_NAME}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDeSDNV6d4Xp",
        "outputId": "b796a371-e7c5-49a0-d676-625c84b95966"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File size: 1236.985 Kilobytes\n"
          ]
        }
      ],
      "source": [
        "# Print size of pipeline_int8_model_data.cpp\n",
        "convert_bytes(get_file_size(INT8_MODEL_CPP_FILE_NAME), \"KB\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
